dame el codigo tactico estrategico core, importante y configuracion de un rag con ollama (OLLAMA-LANGCHAING-AGENTE) daniel@daniel-laptop:~/tron/programas/TRON/OLLAMA-LANGCHAING-AGENTE/agents/git_specialist$ ollama list
NAME ID SIZE MODIFIED 
qwen2.5:latest 845dbda0ea48 4.7 GB 2 days ago 
glm-4.7:cloud 023608864819 - 2 days ago 
qwen2.5:3b 357c53fb659c 1.9 GB 4 days ago 
llama3.1:8b 46e0c10c039e 4.9 GB 8 days ago 
phi4-mini:latest 78fad5d182a7 2.5 GB 9 days ago 
phi4-mini-reasoning:3.8b 3ca8c2865ce9 3.2 GB 12 days ago 
gemma3:4b a2af6cc3eb7f 3.3 GB 12 days ago 
llama3.2:3b a80c4f17acd5 2.0 GB 12 days ago 
(OLLAMA-LANGCHAING-AGENTE) daniel@daniel-laptop:~/tron/programas/TRON/OLLAMA-LANGCHAING-AGENTE/agents/git_specialist$ 
usando gema de google y con una memoria usando sqlAlchemy

![Assistant Logo](https://chat.langchain.com/_next/image?url=%2Fassets%2Fimages%2FAssistant%20logo-3.png&w=64&q=75)

**Configura tu RAG con Ollama (Gemma 3), LangChain y persistencia en base de datos usando SQLAlchemy.**

Para implementar un agente tÃ¡ctico-estratÃ©gico, debes integrar el modelo localÂ `gemma3:4b`Â medianteÂ `ChatOllama`, definir una herramienta de recuperaciÃ³n (Retriever) y gestionar la memoria de chat conÂ `SQLChatMessageHistory`, que utiliza SQLAlchemy internamente para la persistencia.

```
from langchain_ollama import ChatOllama
from langchain_community.chat_message_histories import SQLChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.tools import tool
from langgraph.prebuilt import create_react_agent

# 1. ConfiguraciÃ³n del Modelo Core (Gemma 3 via Ollama)
llm = ChatOllama(
    model="gemma3:4b",
    temperature=0,
    base_url="http://localhost:11434"
)

# 2. Herramienta TÃ¡ctica de RAG
@tool
def retrieve_docs(query: str):
    """Consulta la base de conocimientos para responder preguntas tÃ©cnicas."""
    # AquÃ­ integrarÃ­as tu vector_store (Chroma, FAISS, etc.)
    # docs = vector_store.similarity_search(query, k=3)
    # return "\n\n".join([d.page_content for d in docs])
    return "InformaciÃ³n recuperada sobre el tema..."

tools = [retrieve_docs]

# 3. ConfiguraciÃ³n de Memoria SQL (SQLAlchemy)
def get_session_history(session_id: str):
    return SQLChatMessageHistory(
        session_id=session_id,
        connection_string="sqlite:///chat_history.db" # SQLAlchemy URI
    )

# 4. CreaciÃ³n del Agente EstratÃ©gico
# Usamos LangGraph para un control superior del flujo
agent_executor = create_react_agent(llm, tools)

# 5. EjecuciÃ³n con Persistencia
config = {"configurable": {"thread_id": "usuario_123"}}

# Ejemplo de invocaciÃ³n
# result = agent_executor.invoke(
#     {"messages": [("user", "ExplÃ­came el cÃ³digo tÃ¡ctico de este RAG")]},
#     config=config
# )
```

Copy

## Arquitectura de Memoria con SQLAlchemy

Para que la memoria sea verdaderamente persistente y escalable,Â `SQLChatMessageHistory`Â actÃºa como el puente hacia cualquier base de datos compatible con SQLAlchemy (PostgreSQL, MySQL, SQLite).

```
# ConfiguraciÃ³n avanzada de la memoria
history = SQLChatMessageHistory(
    session_id="session_001",
    connection_string="postgresql://user:pass@localhost/dbname", # Cambia a tu DB
    table_name="message_store"
)

# Esto guarda automÃ¡ticamente cada mensaje en la tabla especificada
history.add_user_message("Â¿CÃ³mo configuro la memoria?")
history.add_ai_message("Usa SQLChatMessageHistory para persistencia robusta.")
```

Copy

Esta estructura separa laÂ **lÃ³gica de razonamiento**Â (el Agente), laÂ **fuente de verdad**Â (el Retriever/RAG) y laÂ **experiencia del usuario**Â (la Memoria SQL), permitiendo que el sistema sea modular y fÃ¡cil de depurar en entornos de producciÃ³n.

**Relevant docs:**

- [Ollama Integration Guide](https://docs.langchain.com/oss/python/integrations/providers/ollama)
- [SQL Chat Message History](https://docs.langchain.com/oss/python/integrations/memory/sql_chat_message_history)
- [Build a RAG Agent](https://docs.langchain.com/oss/python/langchain/rag)
- [LangGraph Persistence](https://docs.langchain.com/oss/python/langgraph/persistence)



hablemos de RecursiveCharacterTextSplitter de LangChain, es decir quiero deiseÃ±ar una base de datos local de la documentaciÃ³n de cognee, ya la tengo en markdown, y estÃ¡ bien estructurada es decir los # en los tÃ­tulos etc... necesito esta herramienta, es decir que funcionalidad buena me trae no usar oython solo y darle con a mano y picar como yo quiero, que ventajas de usar esta tecnologia? me facilita el trabajo? como ? como se adapta para que sea inteligente ? por otro lado supon que estoy haciendo un rag de documentos en sqlite con documentos completos es decir ya los nÃºcleos de informaciÃ³n estÃ¡n definidos, incluso tengo la ubicaciÃ³n de los nÃºmeros de pÃ¡gina de tÃ­tulos y subtitulos markdown en el documento, cual es la mejor manera de calcular un rag con gema por ejemplo que tengo que considerar respecto al chunk, y si lo uso con ollama local y gema y tambien con deepseek api es decir puedo cambiar el modelo de manera interacticvaen el chat como es eso en la interfaz cli y como en el backend

![Assistant Logo](https://chat.langchain.com/_next/image?url=%2Fassets%2Fimages%2FAssistant%20logo-3.png&w=64&q=75)

Agent stepsÂ (9)â€¢28s

01Planning next steps...

02Searching support articles (OSS)

03Searching documentation for "markdown"

04Searching documentation for "routing"

05Searching documentation for "text splitters"

06Searching documentation for "configurable fields"

07Searching documentation for "markdown splitter"

08Searching documentation for "ollama gemma chunk size"

09check_links

**UsarÂ `RecursiveCharacterTextSplitter`Â te ahorra escribir lÃ³gica compleja de segmentaciÃ³n manualmente porque respeta la jerarquÃ­a natural del texto para mantener el contexto semÃ¡ntico.**

A diferencia de un simpleÂ `split()`Â de Python, esta herramienta utiliza una lista de separadores (comoÂ `\n\n`,Â `\n`,Â ,Â `""`) de forma recursiva. Si un pÃ¡rrafo es demasiado grande, intenta dividirlo por oraciones; si la oraciÃ³n sigue siendo grande, por palabras. Esto garantiza que tus nÃºcleos de informaciÃ³n no se corten a la mitad de una idea importante.

```
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ConfiguraciÃ³n inteligente
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,       # TamaÃ±o mÃ¡ximo del nÃºcleo
    chunk_overlap=100,     # Contexto compartido entre fragmentos
    add_start_index=True   # Guarda la posiciÃ³n original en el documento
)

docs = text_splitter.create_documents([markdown_text])
```

Copy

## Ventaja EspecÃ­fica para Markdown

Dado que ya tienes tu documentaciÃ³n en Markdown bien estructurada, la mejor opciÃ³n no es solo el splitter recursivo, sino elÂ `MarkdownHeaderMetadataSplitter`. Esta herramienta es "inteligente" porque:

1. **Mantiene el Contexto**: Asocia cada fragmento con los tÃ­tulos (`#`,Â `##`,Â `###`) a los que pertenece en los metadatos.
2. **Facilita el RAG**: Cuando recuperas un fragmento, el modelo sabe exactamente de quÃ© secciÃ³n viene (ej: "InstalaciÃ³n > ConfiguraciÃ³n Local").

```
from langchain_text_splitters import MarkdownHeaderMetadataSplitter

headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderMetadataSplitter(headers_to_split_on=headers_to_split_on)
md_header_splits = markdown_splitter.split_text(tu_documento_markdown)
```

Copy

## RAG con Gemma y DeepSeek

Para modelos locales comoÂ **Gemma (vÃ­a Ollama)**Â y APIs potentes comoÂ **DeepSeek**, considera lo siguiente:

- **TamaÃ±o de Chunk**: Gemma suele tener una ventana de contexto mÃ¡s pequeÃ±a (alrededor de 8k). Chunks de 500-800 caracteres son ideales para dejar espacio a las instrucciones del sistema. DeepSeek (128k) aguanta chunks mucho mÃ¡s grandes, pero fragmentos pequeÃ±os suelen mejorar la precisiÃ³n de la bÃºsqueda vectorial.
- **SQLite y Metadatos**: Al guardar en SQLite, asegÃºrate de indexar las columnas de metadatos (pÃ¡gina, tÃ­tulo) para poder filtrar las bÃºsquedas antes de pasarlas al modelo.

## Cambio de Modelo Interactivo (CLI y Backend)

Para cambiar entre Gemma (Local) y DeepSeek (API) de forma dinÃ¡mica, LangChain utiliza la funcionalidad deÂ **campos configurables**.

### En el Backend

UtilizaÂ `init_chat_model`Â para definir un modelo que cambie segÃºn la configuraciÃ³n que envÃ­es en elÂ `invoke`:

```
from langchain.chat_models import init_chat_model

# Definimos un modelo base configurable
model = init_chat_model(
    model="gemma2",
    model_provider="ollama",
    configurable_fields=("model", "model_provider"),
)

# InvocaciÃ³n local (Gemma)
res1 = model.invoke("Hola", config={"configurable": {"model": "gemma2", "model_provider": "ollama"}})

# InvocaciÃ³n vÃ­a API (DeepSeek)
res2 = model.invoke("Hola", config={"configurable": {"model": "deepseek-chat", "model_provider": "deepseek"}})
```

Copy

### En el CLI

Puedes manejar esto con un simple argumento de entrada o una variable de estado en tu bucle de chat:

```
# LÃ³gica simple para CLI
selected_model = input("Elige modelo (1: Gemma, 2: DeepSeek): ")
model_name = "gemma2" if selected_model == "1" else "deepseek-chat"

# El backend recibe model_name y lo inyecta en el RunnableConfig
chain.invoke({"input": query}, config={"configurable": {"model": model_name}})
```

Copy

**Relevant docs:**

- [Recursive Character Text Splitter](https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter)
- [Markdown Header Metadata Splitter](https://docs.langchain.com/oss/python/integrations/splitters/markdown_header_metadata_splitter)
- [Configurable Models and Fields](https://docs.langchain.com/oss/python/langchain/models#configurable-models)

28.2sâ€¢43.2k tokensâ€¢$0.0251[View trace](https://smith.langchain.com/public/d4825f7a-e392-4374-b851-499c350ac2ed/r)

CopyGoodBadFeedback

![User](https://chat.langchain.com/_next/image?url=%2Fassets%2Fimages%2FUser%20icon.png&w=64&q=75)

entiendo pero a veces es insuficiente es decir from langchain_text_splitters import RecursiveCharacterTextSplitter

# ConfiguraciÃ³n inteligente

text_splitter = RecursiveCharacterTextSplitter(
 chunk_size=1000, # TamaÃ±o mÃ¡ximo del nÃºcleo
 chunk_overlap=100, # Contexto compartido entre fragmentos
 add_start_index=True # Guarda la posiciÃ³n original en el documento
)
docs = text_splitter.create_documents([markdown_text]) ... en la documentaciÃ³n de programaciÃ³n por ejemplo hay # dentro del cÃ³digo en comentarios por ejemplo y al principio de la linea esta tecnologÃ­a de langchaing no se confunde? tambien hay # cuando en markdown tenemos un enlace que lleva a un tÃ­tulo de otro documento, y eso ocasiona unos dolores de cabeza del infierno, entonces con la documentacion de cognee por ejemplo que es bien estructurada casi no hay problema pero con la sql alchemy si que hay millones de problemas, eso por un lado, por otro lado yo tengo una manera de alimentar la base de datos con los documentos, siempre imagino que hay incluso para sqlalchemy una manera de ir a los tÃ­tulos principales tener un Ã¡rbol de informaciÃ³n de titulos principales y secundarios y todos los documentos en la base de datos en sqlite ero no se como, es decir descargo la documentacion de la internet, ok tengo carpetas ordenasdas por temas ejemplos etc todo en markdown y desde allÃ­ meter todos los documentos en sqlite? con sus metadatos como es decir los tÃ­tulos subtitulos? me sirve de algo hallar los nÃºmeros de lÃ­nea de los documentos donde estÃ¡ el tÃ­tulo? algo como {"addr": "1.2", "title": "DataPoints: Atomic Units of Knowledge", "line": 362, "level": 1},
 {"addr": "2.1", "title": "Core Structure", "line": 385, "level": 2},
 {"addr": "2.2", "title": "From DataPoints to the Graph", "line": 412, "level": 2},
 {"addr": "2.3", "title": "Examples and details", "line": 428, "level": 2},
 {"addr": "2.4", "title": "Main Operations", "line": 518, "level": 2},
 {"addr": "1.1", "title": "Tasks", "line": 542, "level": 1},
 {"addr": "2.1", "title": "Examples and details", "line": 636, "level": 2},
 {"addr": "2.2", "title": "Main Operations", "line": 704, "level": 2},
 {"addr": "1.1", "title": "Pipelines", "line": 728, "level": 1},
 {"addr": "2.1", "title": "How pipelines run", "line": 762, "level": 2},
 {"addr": "2.2", "title": "Main Operations", "line": 808, "level": 2},
 {"addr": "1.1", "title": "Add", "line": 832, "level": 1},
 {"addr": "2.1", "title": "Further details", "line": 910, "level": 2}, ...de un documento maestro o de varios? como organizo los metadatos en este sqlite pensando despues sean valioso en la busqueda vectorial?

![Assistant Logo](https://chat.langchain.com/_next/image?url=%2Fassets%2Fimages%2FAssistant%20logo-3.png&w=64&q=75)

Agent stepsÂ (7)â€¢26s

01Planning next steps...

02Searching documentation for "RAG retrieval"

03Searching documentation for "metadata document"

04Searching documentation for "vector store sqlite"

05Searching documentation for "semantic chunking"

06Searching documentation for "document loader sql"

07Searching documentation for "hierarchical metadata extraction"

Tienes razÃ³n â€”Â **los splitters genÃ©ricos confunden caracteres especiales en cÃ³digo y markdown**. Para SQLAlchemy o docs complejas, necesitas una estrategia mÃ¡s sofisticada. Tu estructura de metadatos (con lÃ­nea, tÃ­tulo jerÃ¡rquico, y nivel) es exactamente lo que necesitas para un RAG robusto.

## El Problema Real

Cuando usasÂ `RecursiveCharacterTextSplitter`Â en documentaciÃ³n con cÃ³digo, losÂ `#`Â en comentarios de Python se interpretan como separadores, rompiendo chunks en mitad de funciones. Peor aÃºn, los enlaces markdown comoÂ `[funciÃ³n](#anchor)`Â generan metadatos incorrectos. NecesitasÂ **pre-procesar el documento respetando su jerarquÃ­a**.

## SoluciÃ³n: Extraer JerarquÃ­a Antes de Dividir

Tu enfoque esÂ **el correcto**. Primero extrae la estructura de tÃ­tulos (como hiciste), luego dividesÂ **dentro de cada secciÃ³n**, preservando contexto jerÃ¡rquico. AsÃ­:

```
import sqlite3
import re
from langchain.schema import Document

# 1. EXTRAER JERARQUÃA DE TÃTULOS
def extract_hierarchy(markdown_text):
    """Extrae tÃ­tulos con lÃ­nea, nivel e Ã­ndice jerÃ¡rquico"""
    sections = []
    lines = markdown_text.split('\n')

    for idx, line in enumerate(lines):
        match = re.match(r'^(#{1,6})\s+(.+)$', line)
        if match:
            level = len(match.group(1))  # NÃºmero de #
            title = match.group(2).strip()
            sections.append({
                "line": idx,
                "title": title,
                "level": level,
                "start_char": len('\n'.join(lines[:idx]))
            })

    return sections

# 2. DIVIDIR DENTRO DE SECCIONES (respetando estructura)
def split_by_sections(markdown_text, max_chunk_size=1000):
    """Divide el texto respetando tÃ­tulos, NO por caracteres ciegos"""
    sections = extract_hierarchy(markdown_text)
    chunks = []
    lines = markdown_text.split('\n')

    for i, section in enumerate(sections):
        start_line = section['line'] + 1  # DespuÃ©s del tÃ­tulo
        end_line = sections[i + 1]['line'] if i + 1 < len(sections) else len(lines)

        section_text = '\n'.join(lines[start_line:end_line])

        # Si la secciÃ³n es muy grande, SOLO ENTONCES divide por pÃ¡rrafos/oraciones
        if len(section_text) > max_chunk_size:
            # Divide por pÃ¡rrafos primero (respeta bloques de cÃ³digo)
            paragraphs = section_text.split('\n\n')
            current_chunk = ""

            for para in paragraphs:
                if len(current_chunk) + len(para) > max_chunk_size:
                    if current_chunk:
                        chunks.append({
                            "content": current_chunk,
                            "parent_title": section['title'],
                            "parent_level": section['level'],
                            "start_line": section['line'],
                            "source": "markdown"
                        })
                    current_chunk = para
                else:
                    current_chunk += '\n\n' + para if current_chunk else para

            if current_chunk:
                chunks.append({
                    "content": current_chunk,
                    "parent_title": section['title'],
                    "parent_level": section['level'],
                    "start_line": section['line'],
                    "source": "markdown"
                })
        else:
            chunks.append({
                "content": section_text,
                "parent_title": section['title'],
                "parent_level": section['level'],
                "start_line": section['line'],
                "source": "markdown"
            })

    return chunks

# 3. GUARDAR EN SQLITE CON METADATOS JERÃRQUICOS
def save_to_sqlite(chunks, db_path="docs.db"):
    """Guarda chunks en SQLite con Ã­ndices para bÃºsqueda rÃ¡pida"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Tabla con Ã­ndices estratÃ©gicos
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS documents (
            id INTEGER PRIMARY KEY,
            content TEXT NOT NULL,
            parent_title TEXT,
            parent_level INTEGER,
            start_line INTEGER,
            source TEXT,
            embedding BLOB,  -- Para bÃºsqueda vectorial luego
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    # Ãndices para bÃºsqueda eficiente
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_parent_title ON documents(parent_title)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_parent_level ON documents(parent_level)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON documents(source)')

    for chunk in chunks:
        cursor.execute('''
            INSERT INTO documents 
            (content, parent_title, parent_level, start_line, source)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            chunk['content'],
            chunk['parent_title'],
            chunk['parent_level'],
            chunk['start_line'],
            chunk['source']
        ))

    conn.commit()
    conn.close()

# USO
with open("cognee_docs.md", "r") as f:
    markdown_content = f.read()

chunks = split_by_sections(markdown_content, max_chunk_size=1000)
save_to_sqlite(chunks)
```

Copy

## Estructura de Metadatos para RAG Efectivo

La clave para RAG inteligente esÂ **guardar metadatos que te permitan filtrar ANTES de buscar vectores**. Tu estructura debe ser asÃ­:

```
{
  "id": 1,
  "content": "...",
  "parent_title": "Core Structure",
  "parent_level": 2,
  "breadcrumb": "DataPoints > Core Structure",  // Ruta completa
  "start_line": 385,
  "doc_source": "cognee_docs.md",
  "embedding": [...],  // Vector despuÃ©s
  "chunk_order": 1  // Para recuperar contexto adyacente
}
```

Copy

## RecuperaciÃ³n Inteligente en RAG

```
from langchain.embeddings import OllamaEmbeddings
import numpy as np

# Embeddings locales con Gemma
embeddings = OllamaEmbeddings(model="gemma2", base_url="http://localhost:11434")

# RecuperaciÃ³n hÃ­brida: PRIMERO filtrar por metadatos, LUEGO vectorial
def hybrid_retrieval(query, top_k=5, db_path="docs.db"):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # OpciÃ³n 1: Si el usuario pregunta por "instalaciÃ³n", filtra por tÃ­tulo
    # OpciÃ³n 2: Si no hay coincidencia de tÃ­tulo, bÃºsqueda vectorial

    query_embedding = embeddings.embed_query(query)

    # Pseudo-SQL (necesitarÃ­as extensiÃ³n SQLite para bÃºsqueda vectorial real)
    # En producciÃ³n usa pgvector en PostgreSQL o FAISS
    cursor.execute('''
        SELECT id, content, parent_title, parent_level, start_line
        FROM documents
        WHERE source = 'cognee_docs.md'
        LIMIT ?
    ''', (top_k,))

    results = cursor.fetchall()
    conn.close()

    return results
```

Copy

## Modelo Configurable (Gemma Local + DeepSeek API)

```
from langchain.chat_models import init_chat_model
from langchain.runnables import RunnableConfig

# Crea un modelo configurable
def get_rag_chain(model_config="local"):
    """
    model_config: "local" (Gemma) o "api" (DeepSeek)
    """

    if model_config == "local":
        llm = init_chat_model(
            model="gemma2",
            model_provider="ollama",
            base_url="http://localhost:11434"
        )
    else:  # api
        llm = init_chat_model(
            model="deepseek-chat",
            model_provider="deepseek",
            api_key="tu-clave"
        )

    # Chain RAG simple
    def retrieve_and_answer(query, config: RunnableConfig = None):
        results = hybrid_retrieval(query)
        context = "\n\n".join([r[1] for r in results])  # Contenido

        prompt = f"""BasÃ¡ndote en esto:

        {context}

        Responde: {query}"""

        return llm.invoke(prompt)

    return retrieve_and_answer

# EN CLI
print("Elige modelo:")
print("1: Gemma (Local)")
print("2: DeepSeek (API)")
choice = input("> ")

model = "local" if choice == "1" else "api"
rag_chain = get_rag_chain(model)

while True:
    query = input("Pregunta: ")
    answer = rag_chain(query)
    print(answer)
```

Copy

## Ventajas de tu Enfoque

1. **No confundeÂ `#`Â en cÃ³digo**: Solo trataÂ `#`Â de markdown reales
2. **Metadatos jerÃ¡rquicos**: Sabes "esto es de la secciÃ³n X nivel 2"
3. **BÃºsqueda filtrada**: Si pides "instalaciÃ³n", busca solo en esa secciÃ³n
4. **RecuperaciÃ³n de contexto**: Puedes traer chunks adyacentes porÂ `chunk_order`
5. **Flexible**: Cambia entre modelos sin reindexar

**Para documentaciÃ³n compleja como SQLAlchemy, un paso previo es crÃ­tico: NO confÃ­es solo en splitters genÃ©ricos.**

26.8sâ€¢55.2k tokensâ€¢$0.0550[View trace](https://smith.langchain.com/public/c90f6133-d499-4452-a32e-ef54eac803e8/r)

CopyGoodBadFeedback

![User](https://chat.langchain.com/_next/image?url=%2Fassets%2Fimages%2FUser%20icon.png&w=64&q=75)

entonces creo que la alimentacion chunking y demas dependen de el tipo de informacion y como estÃ© organizada entonces cada caso es aparte es decir mis clientes como me organizo la informacion de mis clientes de manera sistematica y automatizadasi cada caso es aparte?

![Assistant Logo](https://chat.langchain.com/_next/image?url=%2Fassets%2Fimages%2FAssistant%20logo-3.png&w=64&q=75)

Agent stepsÂ (4)â€¢16s

01Planning next steps...

02Searching documentation for "document transformer"

03Searching documentation for "directory loader"

04Searching documentation for "multi document loader"

**SÃ­, cada tipo de contenido necesita su estrategia especÃ­fica de chunking, pero puedes automatizarlo sistemÃ¡ticamente con un framework deÂ **detectores + pipelines**.**

La clave es crear unÂ **sistema de detecciÃ³n automÃ¡tica**Â que identifique el tipo de documento y aplique el pipeline correcto. AsÃ­ escalas para mÃºltiples clientes sin cÃ³digo manual por cada uno.

## Framework Automatizado: Pipeline Factory

```
import os
from pathlib import Path
from langchain.document_loaders import (
    DirectoryLoader, 
    TextLoader, 
    UnstructuredMarkdownLoader,
    UnstructuredWordDocumentLoader,
    CSVLoader,
    PyMuPDFLoader  # Para PDFs
)
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderMetadataSplitter,
    PythonCodeTextSplitter,
    CharacterTextSplitter
)
from typing import Dict, Any

# 1. PIPELINES POR TIPO DE CONTENIDO
PIPELINES = {
    "markdown_docs": {
        "loader": UnstructuredMarkdownLoader,
        "splitter": "markdown_hierarchy",  # Tu lÃ³gica personalizada
        "chunk_size": 1000,
        "metadata": ["parent_title", "section_level", "start_line"]
    },
    "python_code": {
        "loader": TextLoader,
        "splitter": PythonCodeTextSplitter,  # Respeta funciones/clases
        "chunk_size": 1500,
        "metadata": ["file_path", "function_name", "class_name"]
    },
    "pdf_docs": {
        "loader": PyMuPDFLoader,
        "splitter": RecursiveCharacterTextSplitter,
        "chunk_size": 800,
        "metadata": ["page_number", "source_file"]
    },
    "word_docs": {
        "loader": UnstructuredWordDocumentLoader,
        "splitter": RecursiveCharacterTextSplitter,
        "chunk_size": 1200,
        "metadata": ["source_file"]
    },
    "csv_data": {
        "loader": CSVLoader,
        "splitter": "row_wise",  # Cada fila como chunk
        "chunk_size": 500,
        "metadata": ["row_number", "category"]
    }
}

def detect_document_type(file_path: str) -> str:
    """Detecta automÃ¡ticamente el tipo por extensiÃ³n y contenido"""
    ext = Path(file_path).suffix.lower()

    if ext == ".md":
        return "markdown_docs"
    elif ext in [".py", ".js", ".ts"]:
        return "python_code"
    elif ext == ".pdf":
        return "pdf_docs"
    elif ext == ".docx":
        return "word_docs"
    elif ext == ".csv":
        return "csv_data"
    else:
        # Fallback: inspecciona contenido
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read(5000)
            if 'def ' in content or 'class ' in content:
                return "python_code"
            elif '```' in content:
                return "markdown_docs"
        return "generic"

def get_pipeline(doc_type: str) -> Dict[str, Any]:
    """Devuelve configuraciÃ³n del pipeline"""
    return PIPELINES.get(doc_type, PIPELINES["generic"])

# 2. FACTORY DE SPLITTERS INTELIGENTES
def create_splitter(config: Dict) -> Any:
    """Crea el splitter correcto segÃºn tipo"""
    splitter_type = config.get("splitter")

    if splitter_type == "markdown_hierarchy":
        # Tu lÃ³gica personalizada de jerarquÃ­a
        return MarkdownHierarchySplitter(config["chunk_size"])
    elif splitter_type == "python_code":
        return PythonCodeTextSplitter(chunk_size=config["chunk_size"])
    elif splitter_type == "row_wise":
        return CharacterTextSplitter(separator="\n", chunk_size=config["chunk_size"])
    else:
        return RecursiveCharacterTextSplitter(
            chunk_size=config["chunk_size"], 
            chunk_overlap=100
        )

class MarkdownHierarchySplitter:
    """Tu splitter personalizado para markdown"""
    def __init__(self, chunk_size):
        self.chunk_size = chunk_size

    def split_text(self, text):
        return split_by_sections(text, self.chunk_size)  # Tu funciÃ³n anterior

# 3. PIPELINE PRINCIPAL AUTOMATIZADO
def process_directory(input_dir: str, output_db: str):
    """Procesa TODO el directorio automÃ¡ticamente"""
    loader_map = {}

    for file_path in Path(input_dir).rglob("*"):
        if file_path.is_file():
            doc_type = detect_document_type(str(file_path))
            loader_class = get_pipeline(doc_type)["loader"]
            loader_map[str(file_path)] = loader_class()

    # Carga TODO con DirectoryLoader mapeado
    loader = DirectoryLoader(
        input_dir, 
        glob="**/*", 
        loader_cls=TextLoader,  # Fallback
        loader_kwargs={"encoding": "utf-8"},
        show_progress=True
    )

    docs = loader.load()

    # Procesa cada doc con su pipeline especÃ­fico
    all_chunks = []
    for doc in docs:
        doc_type = detect_document_type(doc.metadata["source"])
        config = get_pipeline(doc_type)
        splitter = create_splitter(config)

        chunks = splitter.split_documents([doc])

        # AÃ±ade metadatos especÃ­ficos
        for chunk in chunks:
            chunk.metadata.update({
                "doc_type": doc_type,
                "client": extract_client_name(input_dir),  # Cliente del directorio
                **config["metadata"]
            })

        all_chunks.extend(chunks)

    # Guarda en SQLite (tu funciÃ³n anterior)
    save_to_sqlite(all_chunks, output_db)

    print(f"âœ… Procesados {len(all_chunks)} chunks para cliente {input_dir}")

def extract_client_name(dir_path: str) -> str:
    """Extrae nombre del cliente del directorio"""
    return Path(dir_path).name

# USO PARA MÃšLTIPLES CLIENTES
client_dirs = [
    "./clientes/acme/docs/",
    "./clientes/beta_corp/code/",
    "./clientes/gamma/pdf_guides/"
]

for client_dir in client_dirs:
    process_directory(client_dir, f"{client_dir}_knowledge.db")
```

Copy

## Pipeline de IngestiÃ³n para Clientes

```
ğŸ“ clientes/
â”œâ”€â”€ acme/           # Docs markdown + cÃ³digo
â”‚   â””â”€â”€ knowledge.db
â”œâ”€â”€ beta_corp/      # PDFs + Word
â”‚   â””â”€â”€ knowledge.db
â””â”€â”€ gamma/          # CSV + Python
    â””â”€â”€ knowledge.db
```

Copy

## Comando CLI para Clientes

```
# clients_ingest.py
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--client", help="Nombre del cliente")
parser.add_argument("--dir", help="Directorio de documentos")

args = parser.parse_args()

if args.client:
    process_directory(f"./clientes/{args.client}/", f"./clientes/{args.client}/knowledge.db")
```

Copy

```
# Para un cliente nuevo
python clients_ingest.py --client acme --dir "./acme_docs/"

# Para todos
for client in clientes/*/; do
    python clients_ingest.py --client $(basename $client)
done
```

Copy

## Ventajas del Sistema

1. **AutomÃ¡tico**: Detecta tipo y aplica pipeline correcto
2. **Escalable**: Nuevo tipo? Solo aÃ±ade aÂ `PIPELINES`
3. **Multi-cliente**: Base de datos separada por cliente
4. **Metadatos ricos**:Â `client`,Â `doc_type`, jerarquÃ­a
5. **Fallback seguro**: Si no detecta, usa genÃ©rico

## RAG Multi-Cliente

```
def multi_client_rag(query, client_name: str):
    """RAG especÃ­fico por cliente"""
    db_path = f"./clientes/{client_name}/knowledge.db"
    # Tu lÃ³gica de retrieval con filtro client=client_name
```

Copy

**Esto te da un sistema enterprise**: cada cliente sube docs, tÃº corres el pipeline una vez, y tienes RAG listo. Â¡Sin cÃ³digo manual por cliente!**

**Relevant docs:**

- [DirectoryLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/directory)
- [Document Transformers](https://docs.langchain.com/oss/python/integrations/document_transformers/)
- [Python Code Splitter](https://docs.langchain.com/oss/python/integrations/splitters/python)

**Tu pipeline actual (extraer tÃ­tulos conÂ `addr/line/level`Â y luego indexar) es una base sÃ³lida y moderna; vale la pena mantenerlo pero aÃ±adir tres mejoras clave â€”ignorar bloques de cÃ³digo, calcularÂ `start_char`/`end_char`Â y generarÂ `breadcrumb`Â jerÃ¡rquicoâ€” para convertirlo en un RAG robusto y escalable.**

Te explico por quÃ© y cÃ³mo llevarlo a â€œwowâ€: primero extraes la estructura (ya lo haces), luego limpias/filtras para evitar falsos positivos (headers dentro de code fences), construyes la jerarquÃ­a padreâ†’hijo y breadcrumbs, divides por secciÃ³n (solo si es necesario) usando splitters recursivos, generas embeddings y guardas chunks con metadatos ricos para poder filtrar antes de buscar.

```
# 1) Ignorar bloques de cÃ³digo y extraer headers con posiciÃ³n exacta
import re

def mask_fenced_code(text):
    """Reemplaza temporalmente el contenido de los fences ```...``` por marcas para evitar detectar headers allÃ­."""
    return re.sub(r'```.*?```', lambda m: '\n'.join(['<CODE_BLOCK>']*m.group(0).count('\n')), text, flags=re.DOTALL)

def extract_headers_with_positions(md_text):
    """Devuelve lista de {addr, title, line, level, start_char} ignorando headers dentro de fences."""
    masked = mask_fenced_code(md_text)
    lines = masked.splitlines(keepends=True)
    counters = {}
    sections = []
    char_pos = 0
    for i, line in enumerate(lines, start=1):
        m = re.match(r'^(#{1,6})\s+(.+)$', line)
        if m:
            level = len(m.group(1))
            title = m.group(2).strip()
            # manejar contadores jerÃ¡rquicos
            for l in range(level+1, 7):
                counters[l] = 0
            counters[level] = counters.get(level, 0) + 1
            addr = f"{level}.{counters[level]}"
            sections.append({
                "addr": addr,
                "title": title,
                "line": i,
                "level": level,
                "start_char": char_pos  # posiciÃ³n en caracteres desde inicio del doc
            })
        char_pos += len(line)
    return sections
```

Copy

## CÃ³mo chunkear respetando tuÂ `addr`Â / jerarquÃ­a

Divide por secciÃ³n (cada header define una secciÃ³n que termina donde empieza la siguiente). Dentro de cada secciÃ³n:

- Si la secciÃ³n es corta, guÃ¡rdala como un chunk Ãºnico.
- Si es larga, divide por pÃ¡rrafos (respetando bloques de cÃ³digo) y, si aÃºn es necesario, usa elÂ `RecursiveCharacterTextSplitter`Â conÂ `add_start_index=True`Â yÂ `chunk_overlap`Â razonable.

```
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=900,        # ajusta segÃºn ventana del modelo
    chunk_overlap=150,
    add_start_index=True
)

def split_section_text(section_text):
    # preferir partir por pÃ¡rrafos; fallback al splitter recursivo si hace falta
    paragraphs = [p for p in section_text.split('\n\n') if p.strip()]
    chunks = []
    cur = ""
    for p in paragraphs:
        if len(cur) + len(p) > splitter.chunk_size:
            if cur:
                chunks.extend(splitter.split_text(cur))
            cur = p
        else:
            cur = cur + "\n\n" + p if cur else p
    if cur:
        chunks.extend(splitter.split_text(cur))
    return chunks
```

Copy

## Metadatos que debes almacenar en SQLite (imprescindible para bÃºsquedas Ãºtiles)

- `doc_id`,Â `source_path`,Â `file_name`
- `section_addr`,Â `section_title`,Â `section_level`
- `breadcrumb`Â (ruta completa ej.Â `"Architecture > DataPoints > Core Structure"`)
- `start_line`,Â `end_line`,Â `start_char`,Â `end_char`
- `chunk_order`,Â `chunk_text`,Â `chunk_tokens`Â (opcional),Â `chunk_size_chars`
- `anchors`Â (lista de ids/links internos),Â `page_number`Â (si viene de PDF)
- `embedding`Â (vector almacenado o referencia al vectorstore)
- Ã­ndices:Â `doc_id`,Â `source_path`,Â `section_title`,Â `section_level`, y columnas para filtrar por cliente/proyecto

Ejemplo esquema SQL simplificado:

```
CREATE TABLE docs (
  id INTEGER PRIMARY KEY,
  doc_id TEXT,
  source_path TEXT,
  section_addr TEXT,
  section_title TEXT,
  section_level INTEGER,
  breadcrumb TEXT,
  start_line INTEGER,
  end_line INTEGER,
  start_char INTEGER,
  end_char INTEGER,
  chunk_order INTEGER,
  chunk_text TEXT,
  embedding BLOB,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
CREATE INDEX idx_breadcrumb ON docs(breadcrumb);
CREATE INDEX idx_section_title ON docs(section_title);
```

Copy

## Evitar falsos positivos (# dentro de cÃ³digo, enlaces, frontmatter)

- Ignora headers dentro de fences triple-backtick y ofuscadosÂ `<!-- -->`Â o bloques HTML.
- Ignora YAML frontmatter (encabezado entreÂ `---`Â que no es contenido).
- Usa la detecciÃ³n de code fences y mÃ¡scaras antes de extraer headers (verÂ `mask_fenced_code`Â arriba).
- Para anchors internos y enlacesÂ `[...] (#anchor)`, no los trates como headers: tu regex ya exigeÂ `^#`Â al inicio de la lÃ­nea, pero presta atenciÃ³n a lÃ­neas de comentarios en ejemplo de cÃ³digo que empiecen en columna 0 (por eso mascar code fences).

## Â¿Dejar tu pipeline o migrar a otro?

MantÃ©n tu pipeline: tuÂ `extracted_titles_index_v5.json`Â y los scriptsÂ `build_enhanced_index.py`Â /Â `generar_indice_cognee.py`Â son muy Ãºtiles â€” representan metadatos valiosos. No los descartes; mejÃ³ralos con:

- Pre-procesado para ignorar code fences y frontmatter.
- CÃ¡lculo deÂ `start_char`/`end_char`Â para enlazar chunks exactamente al origen.
- Breadcrumbs y relaciones padreâ†’hijo (Ãºtiles para filtros precisos).
- Si quieres â€œwowâ€, aÃ±ade: (a) embeddings + vectorstore, (b) recuperaciÃ³n hÃ­brida: metadata filter â†’ vector search â†’ LLM rerank.

## TamaÃ±o de chunk y modelos (Gemma local / DeepSeek / otros)

- AjustaÂ `chunk_size`Â al contexto Ãºtil del modelo: para modelos locales pequeÃ±os usa chunks mÃ¡s cortos (p. ej. 500â€“1,000 chars) para dejar espacio a prompt + contexto; para modelos con contexto largo (o si haces in-context retrieval) puedes aumentar.
- UsaÂ `chunk_overlap`Â (100â€“200 chars) para no cortar ideas.
- Flujo recomendado: primero filtrar porÂ `breadcrumb`/`section_title`Â si la query sugiere una secciÃ³n; si no hay filtro, usa bÃºsqueda vectorial sobre embeddings.

## Flujo RAG recomendado (arquitectura)

1. Ingesta â†’ extracciÃ³n headers â†’ chunks â†’ embeddings.
2. IndexaciÃ³n en vectorstore (FAISS/Chroma/pgvector o SQLite con extensiÃ³n) + metadatos en tabla.
3. Query: detectar intenciÃ³n (Â¿se refiere a secciÃ³n X?) â†’ aplicar filtro por metadatos â†’ recuperar top-K vectores â†’ (opcional) rerank con LLM â†’ construir prompt con contexto y responder.

## Ideas â€œwowâ€ a partir de lo que ya tienes

- Interfaz que muestre el chunk y el breadcrumb + link que abre el archivo en la lÃ­nea original (`start_line`) â€” excelente experiencia de auditorÃ­a.
- RecuperaciÃ³n por secciÃ³n + â€œexpandir contexto adyacenteâ€ (trae chunks prev/next porÂ `chunk_order`).
- Feedback loop: si el usuario marca una respuesta como Ãºtil, guarda esa queryâ†’chunk como seÃ±al de relevancia para futuros rankings.
- Exportar map de contenidos (tu indice markdown) como sitemap navegable y panel de navegaciÃ³n en UI.

**Relevant docs:**

- [Recursive Character Text Splitter](https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter)
- [Markdown Header Metadata Splitter](https://docs.langchain.com/oss/python/integrations/splitters/markdown_header_metadata_splitter)
- [Text splitters (overview)](https://docs.langchain.com/oss/python/integrations/splitters/index)
- [Build a semantic search engine / knowledge base](https://docs.langchain.com/oss/python/langchain/knowledge-base)

74.4sâ€¢68.0k tokensâ€¢$0.0137[View trace](https://smith.langchain.com/public/b27c1b73-c443-4ca2-a67f-8595a1d4bee5/r)

CopyGoodBadFeedback

![User](https://chat.langchain.com/_next/image?url=%2Fassets%2Fimages%2FUser%20icon.png&w=64&q=75)

y si es un solo documento, pensemos que estoy trabajando de dos maneras 1 el swtche-pipelines de ingesta por tipo de documento aparte y de un pipeline llega el documento cognee.md con toda la informacion de la documentacion y por otro lado el indice...[
 {"addr": "1.1", "title": "Installation", "line": 3, "level": 1},
 {"addr": "2.1", "title": "Prerequisites", "line": 19, "level": 2},
 {"addr": "2.2", "title": "Setup", "line": 63, "level": 2},
 {"addr": "1.1", "title": "Quickstart", "line": 89, "level": 1},
 {"addr": "2.1", "title": "Basic Usage", "line": 103, "level": 2},
 {"addr": "2.2", "title": "About `async` / `await` in Cognee", "line": 163, "level": 2},
 {"addr": "2.3", "title": "Cognee core concepts", "line": 186, "level": 2},
 {"addr": "1.1", "title": "Overview", "line": 196, "level": 1},
 {"addr": "2.1", "title": "Introduction", "line": 208, "level": 2},
 {"addr": "2.2", "title": "Table of Contents", "line": 224, "level": 2},
 {"addr": "2.3", "title": "Next steps", "line": 280, "level": 2},
 {"addr": "1.1", "title": "Architecture", "line": 286, "level": 1},
 {"addr": "2.1", "title": "Why multiple stores", "line": 306, "level": 2},
 {"addr": "2.2", "title": "What is stored where", "line": 322, "level": 2},
 {"addr": "2.3", "title": "How they are used", "line": 338, "level": 2},
 {"addr": "1.1", "title": "DataPoints", "line": 350, "level": 1},
 {"addr": "1.2", "title": "DataPoints: Atomic Units of Knowledge", "line": 362, "level": 1},
 {"addr": "2.1", "title": "Core Structure", "line": 385, "level": 2},
 {"addr": "2.2", "title": "From DataPoints to the Graph", "line": 412, "level": 2},
 {"addr": "2.3", "title": "Examples and details", "line": 428, "level": 2},
 {"addr": "2.4", "title": "Main Operations", "line": 518, "level": 2},
 {"addr": "1.1", "title": "Tasks", "line": 542, "level": 1},
 {"addr": "2.1", "title": "Examples and details", "line": 636, "level": 2},
 {"addr": "2.2", "title": "Main Operations", "line": 704, "level": 2},
 {"addr": "1.1", "title": "Pipelines", "line": 728, "level": 1},
 {"addr": "2.1", "title": "How pipelines run", "line": 762, "level": 2},
 {"addr": "2.2", "title": "Main Operations", "line": 808, "level": 2},
 {"addr": "1.1", "title": "Add", "line": 832, "level": 1},
 {"addr": "2.1", "title": "Further details", "line": 910, "level": 2},
 {"addr": "2.2", "title": "Building Blocks", "line": 962, "level": 2},
 {"addr": "1.1", "title": "Cognify", "line": 986, "level": 1},
 {"addr": "2.1", "title": "What is the cognify operation", "line": 998, "level": 2},
 {"addr": "2.2", "title": "What happens under the hood", "line": 1015, en json que dice que hay en el documento y en que linea, ok el problema es es que en vivo y directo la misma informacion debe 1 ser leida por ollama gemma3:4b y otro dia que quiera algo mas complejo switche a deepseekchat a api y otro dia a un modelo de openrouter entonces fijate que los numeros de linea ya son shuks naturales pero quizas demasiado grandes para gema demasiado pequeÃ±os para Gemma 3 tiene una gran ventana de contexto de 128 K creo que deepseek 128K pero y si quiero usar la api de gemini con open ruuter como se usa una sola chunqueada para los diferentes modelos

![Assistant Logo](https://chat.langchain.com/_next/image?url=%2Fassets%2Fimages%2FAssistant%20logo-3.png&w=64&q=75)

Agent stepsÂ (4)â€¢37s

01Planning next steps...

02Searching support articles (OSS (LangChain and LangGraph),SDKs and APIs)

03Searching documentation for "text splitters"

04Searching documentation for "token"

**SÃ­ â€” puedes usar un Ãºnico documentoÂ `cognee.md`Â y soportar mÃºltiples modelos cambiando la estrategia en tiempo de consulta: guarda chunks finos + metadatos y arma contextos dinÃ¡micamente segÃºn la ventana/token limit del modelo seleccionado.**

Lo esencial: no intentes crear un â€œÃºnico chunkâ€ Ã³ptimo para todos los modelos. En su lugar crea una representaciÃ³n multiâ€‘resoluciÃ³n (chunks atÃ³micos pequeÃ±os + agregados por secciÃ³n) y en tiempo de consulta ensamblas (concatenate) hasta llenar la ventana del modelo objetivo. GuardaÂ `start_char`/`end_char`,Â `start_line`,Â `breadcrumb`Â yÂ `chunk_order`Â para poder reconstruir contexto exacto y traer chunks adyacentes.

## Resumen de la arquitectura prÃ¡ctica

- Ingesta: parseasÂ `cognee.md`Â â†’ extracciÃ³n de headers (ignorando fences de cÃ³digo) â†’ generas chunks pequeÃ±os (pÃ¡rrafos) y opcionalmente chunks agregados por secciÃ³n.
- IndexaciÃ³n: calculas embeddings por chunk (elige un embedding model estable) y guardas en vectorstore + metadatos en SQLite.
- Consulta: al recibirÂ `query`Â +Â `model_choice`Â (ej.Â `gemma_local`,Â `deepseek`,Â `gemini_via_openrouter`), recuperas top-K por vector search (y/o filtrado por metadata), luego ensamblas chunks en orden hasta cumplirÂ `max_context_tokens`Â del modelo, dejando margen para prompt/instrucciones. EnvÃ­o al LLM seleccionado.

## Por quÃ© esto funciona para Gemma / DeepSeek / Gemini

- Modelos con ventanas grandes (DeepSeek 128K, Gemini con contexto amplio): puedes enviar mÃ¡s chunks concatenados sin recomputar indexaciÃ³n.
- Modelos con ventanas pequeÃ±as (modelos locales pequeÃ±os): ensamblas menos chunks o usas chunks mÃ¡s cortos.
- Solo necesitas reindexar si cambias el embedding model; si usas un embedding estable (p. ej. un modelo de embeddings universal), puedes cambiar el LLM sin reingestar.

## CÃ³digo ejemplo (ingesta mÃ­nima + retrieval dinÃ¡mico)

```
# ingest_and_store.py (concepto)
from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter
from langchain.embeddings import OllamaEmbeddings  # ejemplo local
import sqlite3
import tiktoken  # opcional para conteo tokens (OpenAI-like)

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800, chunk_overlap=120, add_start_index=True
)

def ingest_single_doc(path, db_path="docs.db"):
    text = open(path, encoding="utf-8").read()
    # OJO: antes mascara code-fences para no extraer headers dentro de code
    # (ver funciÃ³n mask_fenced_code en la respuesta anterior)
    docs = splitter.create_documents([text], metadatas=[{"source": path}])
    # calcular embeddings y guardar en sqlite (embedding como blob o referencia a vectorstore)
    # ... (simplificado)
```

Copy

```
# retrieval_dynamic.py
def count_tokens(text, encoding_name="cl100k_base"):
    try:
        enc = tiktoken.get_encoding(encoding_name)
        return len(enc.encode(text))
    except Exception:
        # fallback heurÃ­stico: 4 chars ~ 1 token
        return max(1, len(text) // 4)

def assemble_context(chunks, model_max_tokens, prompt_overhead=1500, encoding_name="cl100k_base"):
    """
    chunks: lista ordenada por score/recency/orden en doc; cada chunk tiene 'text' y 'token_count' (precomputed si posible)
    Devuelve concatenaciÃ³n que cabe en model_max_tokens - prompt_overhead
    """
    limit = model_max_tokens - prompt_overhead
    context = []
    used = 0
    for c in chunks:
        tok = c.get("token_count") or count_tokens(c["text"], encoding_name)
        if used + tok > limit:
            break
        context.append(c)
        used += tok
    return "\n\n".join([c["text"] for c in context]), used

# Uso en flow:
# 1) query -> embed -> vector search -> obtener top_N chunks (ordenados por score)
# 2) assembly = assemble_context(top_chunks, model_max_tokens)
# 3) prompt = construir prompt con assembly[0]
# 4) invocar el modelo seleccionado
```

Copy

## Opciones prÃ¡cticas (elige una o combina)

- Tiered chunks (recomendado): almacenar 3 niveles
  - level_small: pÃ¡rrafos atÃ³micos (mejor para ensamblado dinÃ¡mico)
  - level_section: secciones completas (rÃ¡pida para modelos grandes)
  - level_top: capÃ­tulos/headers (para contextos largos y resÃºmenes)
- Solo-small + assemble: almacenar solo chunks pequeÃ±os y ensamblar dinÃ¡micamente (menos duplicaciÃ³n de embeddings).
- PrecomputeÂ `token_count`Â por chunk para acelerar el ensamblado.

## Embeddings y compatibilidad

- Ideal: usar un embedding model estable y reproducible (p. ej. modelos de embeddings abiertos o un proveedor Ãºnico). AsÃ­ no reindexas cuando cambias LLM.
- Si un proveedor impone su embedding, guardas embeddings especÃ­ficos por proveedor en columnas separadas o recomputas sÃ³lo si realmente mejoras calidad.

## Consideraciones sobre tokenizers y ventanas

- Usa un token counter apropiado para cada familia de modelos si precisas mÃ¡xima exactitud (p. ej.Â `tiktoken`Â para OpenAI-like; algunos proveedores ofrecen tokenizers propios).
- Deja margen paraÂ `prompt_overhead`Â (sistema + historial + instrucciones). Ejemplo: si DeepSeek admite 128k, usa 128k - 4k overhead = lÃ­mite Ãºtil.
- Para Gemma local, verifica su contexto mÃ¡ximo y si su tokenizer difiere; si no tienes tokenizer, usa heurÃ­stica charâ†’token conservadora.

## Flujo para cambiar modelos en CLI/back-end

- MantÃ©n un objetoÂ `model_profiles`Â con:Â `name`,Â `provider`,Â `max_tokens`,Â `encoding_name`,Â `prompt_overhead`.
- En CLI: usuario eligeÂ `--model gemma`Â oÂ `--model deepseek`Â â†’ back-end pasaÂ `model_profile`Â al ensamblador y al invocador del LLM.
- No necesitas reindexar; solo el ensamblado y la invocaciÃ³n cambian.

## Ventajas y trade-offs

- Ventajas: Ãºnico Ã­ndice reutilizable, control fino sobre contexto, puedes optimizar por costo/latencia al cambiar modelo.
- Trade-offs: mÃ¡s lÃ³gica en runtime (ensamblado), almacenamiento ligeramente mayor si haces tiered chunks o guardas embeddings por proveedor.

**Relevant docs:**

- [Recursive Character Text Splitter](https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter)
- [Markdown Header Metadata Splitter](https://docs.langchain.com/oss/python/integrations/splitters/markdown_header_metadata_splitter)
- [Text splitters (overview)](https://docs.langchain.com/oss/python/integrations/splitters/index)
- [Build a semantic search engine / knowledge base](https://docs.langchain.com/oss/python/langchain/knowledge-base)
