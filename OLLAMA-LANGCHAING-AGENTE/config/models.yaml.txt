# OLLAMA-LANGCHAING-AGENTE/config/models.yaml

default_model: gemma-ollama

models:
  - id: gemma-ollama
    name: "Gemma (Ollama)"
    alias: gema
    provider: ollama
    config:
      # IMPORTANTE: El nombre del modelo debe coincidir exactamente con 'ollama list'
      model: "gemma3:4b"  # Asegúrate de que este modelo existe con 'ollama list'
      # IMPORTANTE: Definir explícitamente la URL y puerto
      base_url: "http://localhost:11434"
      temperature: 0.7

  - id: deepseek-chat
    name: "DeepSeek (API)"
    alias: deepseek
    provider: deepseek
    config:
      model: "deepseek-chat"
      api_key: "ENV" # Se cargará desde el entorno
